# ğŸ©º Medical Sentiment Analysis

Medical Sentiment Analysis is an end-to-end NLP system that classifies clinical / patient-style text into sentiment (for example: **negative**, **neutral**, **positive**).

* Fine-tuned sentiment model trained on medical-style language (not generic tweets/movie reviews).
* Lightweight HTML interface for entering text and seeing predictions instantly.
* Python backend that serves the model for real-time inference.
* Saved artifacts (tokenizer, label encoder, model weights) so results are reproducible.
* Training notebook that documents how the model was built and evaluated.

---

## ğŸ”´ Demo

<p align="center">
  <video src="https://github.com/user-attachments/assets/71154c8a-f665-4bc6-9ee1-4998f2c30713"
         width="480"
         controls
         muted
         playsinline>
  </video>
</p>

**What the demo shows:**

1. User types a medical-style note / message / complaint.
2. Clicks analyze.
3. The app returns a sentiment label.
4. The UI highlights that label so you can quickly read tone.

This is useful for triage (urgent vs calm), patient experience, and QA.

---

## ğŸ§  Why this matters

Healthcare and clinical communication has its own language.

Examples:

* â€œPatient denies chest pain post-procedureâ€
* â€œFamily is extremely upset about wait timeâ€
* â€œPain is improving and breathing is stable nowâ€

Generic sentiment models fail on that type of phrasing.

This project specifically:

1. Cleans and normalizes text in a medical context.
2. Runs a fine-tuned sentiment classifier.
3. Returns the sentiment class + confidence so you can quickly understand tone.

Real use cases:

* Flag negative / urgent language in patient messages.
* Monitor satisfaction and frustration trends over time.
* Assist staff in prioritizing outreach.

---

## ğŸ— System Overview (What runs where)

```text
[ interface_sentiment.html ]  <-- browser UI for humans
        |
        |  calls the backend with the text you entered
        v
[ server_sentiment/server_sentiment.py ]  <-- Python API service
        |
        |  loads trained model + tokenizer + label encoder
        v
[ server_sentiment/model_artifacts/ ]  <-- all the saved model pieces
    - sentiment_model/            (model config + tokenizer vocab, etc.)
    - label_encoder.pkl           (maps class index -> "negative"/"neutral"/"positive")
```

Plus:

* `NLP_Fine_Tuned_clean.ipynb` documents model training and evaluation.
* `logs.csv` (in `server_sentiment/`) can track requests/results during testing.

So the pipeline is:

**User types text â†’ frontend sends it â†’ backend preprocesses â†’ model predicts â†’ frontend shows label.**

---

## âœ¨ Core Features

### ğŸ©» Domain-tuned sentiment model

* The model is fine-tuned specifically for healthcare-style language instead of casual social text.
* This reduces false positives like:

  * â€œpatient denies painâ€ = actually good/stable, not â€œnegative mood,â€ etc.

### ğŸ”¬ Saved inference artifacts

In `server_sentiment/model_artifacts/` youâ€™ve checked in everything needed for repeatable inference:

* `sentiment_model/`

  * `config.json` â€“ model architecture / config
  * `tokenizer.json`, `tokenizer_config.json`, `special_tokens_map.json`, `vocab.txt` â€“ tokenizer vocab + rules
* `label_encoder.pkl` â€“ converts numeric prediction â†’ readable label (e.g. 0 â†’ "negative")

This proves the model isnâ€™t magic / hardcoded. Itâ€™s actually loaded from trained weights.

### ğŸŒ Live API server

`server_sentiment/server_sentiment.py` runs a backend service (Flask / FastAPI style).
Typical flow:

* Receive text from the client via `POST`.
* Clean/tokenize it using the same tokenizer that was used during training.
* Run inference with the fine-tuned model.
* Return JSON like:

  ```json
  {
    "sentiment": "negative",
    "confidence": 0.91
  }
  ```

This makes your model usable by *other* systems, not just the demo page.

### ğŸ–¥ Frontend interface

* `interface_sentiment.html` is the UI for humans.
* It's a simple webpage where you paste or type text.
* You click a button (Analyze / Classify).
* It hits the backend and then shows the predicted sentiment clearly (e.g. colored badge â€œNEGATIVEâ€ / â€œPOSITIVEâ€).

This looks like a real product, not just a notebook screenshot.

### ğŸ““ Training notebook

* `NLP_Fine_Tuned_clean.ipynb` (and/or `NLP_Fine_Tuned.ipynb`) is the notebook that:

  * loads/cleans training data,
  * tokenizes it / encodes labels,
  * fine-tunes the model,
  * evaluates accuracy,
  * exports the final artifacts that the server uses.

That notebook = proof of work.

---

## ğŸ“‚ Repository structure

Hereâ€™s what your repo currently looks like based on your layout:

```text
medicalsentimentanalysis/
â”œâ”€ README.md
â”œâ”€ interface_sentiment.html          # Frontend UI for entering text + seeing sentiment

â”œâ”€ NLP_Fine_Tuned_clean.ipynb        # Notebook: data prep, training, evaluation, export
#  (You may also have NLP_Fine_Tuned.ipynb)

â”œâ”€ server_sentiment/                 # Backend service + model artifacts
â”‚   â”œâ”€ server_sentiment.py           # API server for inference
â”‚   â”œâ”€ requirements.txt              # All Python deps (Flask/FastAPI, transformers, etc.)
â”‚   â”œâ”€ logs.csv                      # Optional log of predictions / test runs
â”‚   â”‚
â”‚   â””â”€ model_artifacts/              # Saved model + preprocessing
â”‚       â”œâ”€ label_encoder.pkl         # Converts numeric class -> sentiment label string
â”‚       â”‚
â”‚       â””â”€ sentiment_model/          # Model + tokenizer assets
â”‚           â”œâ”€ config.json
â”‚           â”œâ”€ tokenizer.json
â”‚           â”œâ”€ tokenizer_config.json
â”‚           â”œâ”€ special_tokens_map.json
â”‚           â”œâ”€ vocab.txt
â”‚           â””â”€ (other model weight files if present)
```

### Why this structure is nice

* `interface_sentiment.html` = presentation layer
* `server_sentiment/` = serving layer
* `model_artifacts/` = ML assets (frozen snapshot of the trained model)
* Notebook at root = â€œhow I built it / prove I trained itâ€

That separation is exactly what people want to see in applied ML work: train â†’ freeze â†’ serve.

---

## ğŸ”Œ API behavior (server_sentiment.py)

Your backend script (`server_sentiment.py`) is responsible for:

1. Loading the tokenizer and model from `model_artifacts/sentiment_model/`.
2. Loading `label_encoder.pkl` to convert numeric prediction to text label.
3. Exposing an endpoint (examples, adjust names if different):

### Example request:

```json
{
  "text": "Patient is extremely upset about the long wait and says pain is worse."
}
```

### Example response:

```json
{
  "sentiment": "negative",
  "confidence": 0.93
}
```

Internally itâ€™s doing:

* preprocess/clean â†’ tokenize â†’ model forward pass â†’ argmax â†’ map class index to label â†’ send back JSON.

This is production-style architecture, not just Jupyter.

---

## ğŸ–¥ Frontend flow (interface_sentiment.html)

The HTML interface typically does something like:

1. User types or pastes a note into a text box.
2. JavaScript calls the backend (`fetch('/predict', { text: ... })` style).
3. The response comes back as sentiment + score.
4. The page updates to show:

   * Sentiment label (Negative / Neutral / Positive)
   * Optional probability (confidence)
   * Visual highlight/color for the sentiment

     * red for negative, yellow for neutral, green for positive (for fast triage)

This gives you a â€œmini dashboardâ€ experience for testing the model interactively.

---

## ğŸ“˜ Model training (NLP_Fine_Tuned_clean.ipynb)

The notebook documents the full ML lifecycle:

* Dataset loading (medical-like / patient-style text).
* Cleaning text: lowercasing, stripping noise, maybe handling medical phrases.
* Train/val split.
* Fine-tuning a sentiment model (ex: transformer-based classifier).
* Evaluating performance (accuracy, F1, confusion matrix).
* Exporting:

  * tokenizer files (`tokenizer.json`, `vocab.txt`, etc.),
  * model config/weights (`config.json`, etc.),
  * encoders (`label_encoder.pkl`).

Those exports land in `server_sentiment/model_artifacts/`, and thatâ€™s what the live API loads.

This shows:

* You actually trained a model,
* You froze the artifacts,
* And you wired it into a running UI.

---

## ğŸš€ Run it locally

### 1. Install backend dependencies

```bash
cd server_sentiment
python -m venv venv

# macOS / Linux:
source venv/bin/activate

# Windows:
# venv\Scripts\activate

pip install -r requirements.txt
```

### 2. Start the inference server

```bash
python server_sentiment.py
```

Now your API should be running locally (for example on `http://localhost:5000` or whatever port is in your code).

### 3. Open the interface

Option A (static HTML):

* Just open `interface_sentiment.html` in your browser.
* Make sure that HTML's JavaScript is pointing to the correct backend URL (like `http://localhost:5000/predict`).

Option B (if the HTML is actually served by the backend):

* Visit the route that serves the interface (for example `/`).
* Type a sentence and test your prediction.

---

## ğŸ›£ Roadmap / future improvements

* [ ] Add more nuanced labels (anxious, urgent, reassured, confused).
* [ ] Highlight the exact phrases that drove the classification (â€œupset about wait timeâ€).
* [ ] Batch analysis: upload CSV of patient comments and get a report.
* [ ] Dashboard with sentiment trends over time.
* [ ] Containerize / deploy (so teams can call the API securely without running Python manually).

---

## âš ï¸ Important notes

* This tool is for **sentiment / tone analysis**, not for clinical diagnosis.
* Do not store or commit actual PHI (personal health information).
* `logs.csv` should not contain real patient identifiers in production.
* The notebook is for demonstration of training and evaluation, not for medical decision-making.

---

## TL;DR for reviewers

**Medical Sentiment Analysis =**

* A fine-tuned healthcare sentiment classifier.
* A backend service that exposes it as an API.
* A simple browser UI to test it live.
* Checked-in model artifacts so results are reproducible.
* A training notebook that proves you actually built / trained the model, not just copied something.

This is end-to-end ML product work: collect â†’ train â†’ ship â†’ interact.
