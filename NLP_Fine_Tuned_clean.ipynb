{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies installed.\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "subprocess.run(['apt-get', 'update', '-qq'], check=True)\n",
    "subprocess.run(['apt-get', 'install', '-y', '-qq', 'ffmpeg'], check=True)\n",
    "\n",
    "subprocess.run([\n",
    "    sys.executable, '-m', 'pip', 'install', '-q',\n",
    "    'SpeechRecognition', 'transformers', 'torch', 'datasets',\n",
    "    'scikit-learn', 'evaluate'\n",
    "], check=True)\n",
    "\n",
    "print(\"‚úÖ Dependencies installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3b5ba5614a42688d8d9a12d1e10521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3b658b11cc47608843bb3cb321256e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ab55bfc4a44983a6981b19ceec1abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8729333c277046aaac2587127e8f9e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa94bda0d7b4437a75f230c4292c5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdeea950a7a483caf35532379f65745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1726b74ab54cb59995249f85aba99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='760' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [760/760 03:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.487700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.369600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluation Metrics: {'eval_loss': 0.0004046959220431745, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 1.3484, 'eval_samples_per_second': 225.446, 'eval_steps_per_second': 28.181, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install -q transformers --upgrade\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load and process dataset\n",
    "df = pd.read_csv(\"/content/Dataset_2.csv\")\n",
    "df['sentence'] = df['sentence'].astype(str)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "dataset = Dataset.from_pandas(df[['sentence', 'label']].reset_index(drop=True))\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'test': dataset['test']\n",
    "})\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "encoded_dataset = dataset_dict.map(tokenize_fn, batched=True, remove_columns=[\"sentence\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "# Metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to='none',\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "metrics = trainer.evaluate()\n",
    "print(\"üìä Evaluation Metrics:\", metrics)\n",
    "\n",
    "# Save model & tokenizer\n",
    "model.save_pretrained(\"sentiment_model\")\n",
    "tokenizer.save_pretrained(\"sentiment_model\")\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <button onclick=\"startRecording()\">üé§ Start Recording</button>\n",
       "  <button onclick=\"stopRecording()\">‚èπÔ∏è Stop Recording</button>\n",
       "  <span id=\"status\">Status: Idle</span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(async function() {\n",
       "  let recorder, stream, chunks;\n",
       "  window.startRecording = async () => {\n",
       "    stream = await navigator.mediaDevices.getUserMedia({audio:true});\n",
       "    recorder = new MediaRecorder(stream);\n",
       "    chunks = [];\n",
       "    recorder.ondataavailable = e => chunks.push(e.data);\n",
       "    recorder.onstart = () => {\n",
       "      document.getElementById('status').innerText = 'Status: Recording‚Ä¶';\n",
       "    };\n",
       "    recorder.onstop = () => {\n",
       "      document.getElementById('status').innerText = 'Status: Processing‚Ä¶';\n",
       "      const blob = new Blob(chunks, {type:'audio/webm'});\n",
       "      const reader = new FileReader();\n",
       "      reader.readAsDataURL(blob);\n",
       "      reader.onloadend = () => {\n",
       "        const b64 = reader.result.split(',')[1];\n",
       "        google.colab.kernel.invokeFunction('notebook.saveAudio', [b64], {});\n",
       "        document.getElementById('status').innerText = 'Status: Saved ‚úÖ';\n",
       "      };\n",
       "      stream.getTracks().forEach(t => t.stop());\n",
       "    };\n",
       "    recorder.start();\n",
       "  };\n",
       "  window.stopRecording = () => recorder && recorder.state === 'recording' && recorder.stop();\n",
       "})()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, Javascript, display\n",
    "from google.colab import output\n",
    "import base64\n",
    "import speech_recognition as sr\n",
    "import subprocess\n",
    "\n",
    "# Save audio from browser\n",
    "def _save_audio(b64):\n",
    "    data = base64.b64decode(b64)\n",
    "    with open('recorded.webm', 'wb') as f:\n",
    "        f.write(data)\n",
    "output.register_callback('notebook.saveAudio', _save_audio)\n",
    "\n",
    "# UI buttons\n",
    "display(HTML('''\n",
    "  <button onclick=\"startRecording()\">üé§ Start Recording</button>\n",
    "  <button onclick=\"stopRecording()\">‚èπÔ∏è Stop Recording</button>\n",
    "  <span id=\"status\">Status: Idle</span>\n",
    "'''))\n",
    "\n",
    "# JS to capture audio in browser\n",
    "display(Javascript(\"\"\"\n",
    "(async function() {\n",
    "  let recorder, stream, chunks;\n",
    "  window.startRecording = async () => {\n",
    "    stream = await navigator.mediaDevices.getUserMedia({audio:true});\n",
    "    recorder = new MediaRecorder(stream);\n",
    "    chunks = [];\n",
    "    recorder.ondataavailable = e => chunks.push(e.data);\n",
    "    recorder.onstart = () => {\n",
    "      document.getElementById('status').innerText = 'Status: Recording‚Ä¶';\n",
    "    };\n",
    "    recorder.onstop = () => {\n",
    "      document.getElementById('status').innerText = 'Status: Processing‚Ä¶';\n",
    "      const blob = new Blob(chunks, {type:'audio/webm'});\n",
    "      const reader = new FileReader();\n",
    "      reader.readAsDataURL(blob);\n",
    "      reader.onloadend = () => {\n",
    "        const b64 = reader.result.split(',')[1];\n",
    "        google.colab.kernel.invokeFunction('notebook.saveAudio', [b64], {});\n",
    "        document.getElementById('status').innerText = 'Status: Saved ‚úÖ';\n",
    "      };\n",
    "      stream.getTracks().forEach(t => t.stop());\n",
    "    };\n",
    "    recorder.start();\n",
    "  };\n",
    "  window.stopRecording = () => recorder && recorder.state === 'recording' && recorder.stop();\n",
    "})()\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è Transcript:\n",
      "I want to kill myself\n"
     ]
    }
   ],
   "source": [
    "# Convert to WAV for transcription\n",
    "subprocess.run(['ffmpeg', '-i', 'recorded.webm', 'recorded.wav', '-y'], check=True)\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "with sr.AudioFile('recorded.wav') as src:\n",
    "    audio_data = recognizer.record(src)\n",
    "\n",
    "try:\n",
    "    transcript = recognizer.recognize_google(audio_data)\n",
    "except:\n",
    "    transcript = \"\"\n",
    "\n",
    "print(\"üéôÔ∏è Transcript:\")\n",
    "print(transcript or \"(No speech detected)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìù Transcript Analysis\n",
      "==================================================\n",
      "üìÑ Sentence: I want to kill myself\n",
      "\n",
      "==================================================\n",
      "üìä Sentiment Analysis\n",
      "==================================================\n",
      " ‚Ä¢ Sentiment Detected : negative\n",
      " ‚Ä¢ Confidence Score   : 1.00\n",
      "\n",
      "üö® ALERT: Patient at risk detected based on high negative sentiment!\n",
      "\n",
      "==================================================\n",
      "‚ö†Ô∏è Self-Harm Keyword Detection\n",
      "==================================================\n",
      "üö® Warning: Potential self-harm indicators found!\n",
      " ‚Ä¢ Keyword Detected: \"kill myself\"\n",
      " ‚Ä¢ Keyword Detected: \"want to kill myself\"\n",
      " ‚Ä¢ Keyword Detected: \"kill\"\n",
      "\n",
      "üì¢ ACTION: Please escalate. Recommend contacting crisis support (e.g., Lifeline 13 11 14).\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pickle\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"sentiment_model\"\n",
    "sentiment_analyzer = pipeline(\"text-classification\", model=model_path, tokenizer=model_path)\n",
    "\n",
    "# Load label encoder\n",
    "with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Run sentiment analysis\n",
    "result = sentiment_analyzer(transcript)[0] if transcript else {\"label\": None, \"score\": None}\n",
    "\n",
    "# Decode prediction label\n",
    "if result[\"label\"] and result[\"label\"].startswith(\"LABEL_\"):\n",
    "    label_index = int(result[\"label\"].split(\"_\")[1])\n",
    "    decoded_label = label_encoder.inverse_transform([label_index])[0]\n",
    "else:\n",
    "    decoded_label = None\n",
    "\n",
    "# --- Display Results ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìù Transcript Analysis\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìÑ Sentence: {transcript if transcript else '(No speech detected)'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä Sentiment Analysis\")\n",
    "print(\"=\"*50)\n",
    "if decoded_label:\n",
    "    print(f\" ‚Ä¢ Sentiment Detected : {decoded_label}\")\n",
    "    print(f\" ‚Ä¢ Confidence Score   : {result['score']:.2f}\")\n",
    "\n",
    "    # Check if negative sentiment exceeds 60%\n",
    "    if decoded_label.lower() == \"negative\" and result['score'] > 0.6:\n",
    "        print(\"\\nüö® ALERT: Patient at risk detected based on high negative sentiment!\")\n",
    "else:\n",
    "    print(\" ‚Ä¢ No sentiment detected.\")\n",
    "\n",
    "# --- Self-Harm Keyword Detection ---\n",
    "self_harm_keywords = [\n",
    "     \"suicide\", \"commit suicide\", \"attempt suicide\", \"kill myself\", \"going to kill myself\", \"want to kill myself\", \"thinking about killing myself\", \"off myself\", \"end it all\", \"end my life\",\n",
    "     \"finish it all\", \"finish myself off\", \"hang myself\", \"overdose\", \"overdose on pills\", \"take pills\", \"swallow pills\", \"slit my wrists\", \"slash my arms\", \"cut myself\",\n",
    "     \"self harm\", \"self-harm\", \"hurt myself\", \"want to hurt myself\", \"cause myself pain\", \"i'm worthless\", \"worthless\", \"not worth living\", \"life isn't worth living\", \"no reason to live\",\n",
    "     \"what's the point anymore\", \"feel like dying\", \"feeling suicidal\", \"suicidal thoughts\", \"suicidal ideation\", \"want it to end\", \"wish i were dead\", \"wish i was dead\", \"hope i'm dead\", \"death can't come soon enough\",\n",
    "     \"ready to die\", \"can't go on\", \"can't keep living\", \"don't want to exist\", \"don't want to be here\", \"vanish forever\", \"disappear forever\", \"blow my brains out\", \"shoot myself\", \"put a bullet in my head\",\n",
    "     \"please kill me\", \"kill me\", \"just kill me\", \"drown myself\", \"suffocate myself\", \"strangle myself\", \"crash my car\", \"jump off a bridge\", \"jump off a building\", \"jump in front of a train\",\n",
    "     \"walk in front of traffic\", \"nothing matters\", \"meaningless life\", \"pointless existence\", \"broken beyond repair\", \"beyond saving\", \"can't face tomorrow\", \"everyone would be better off without me\", \"no one would miss me\", \"don't deserve to live\",\n",
    "     \"wasted life\", \"time to die\", \"done with life\", \"done with this\", \"i'm done\", \"my life sucks\", \"life is meaningless\", \"final exit\", \"offing myself\", \"self destruct\", \"self destruction\", \"self electrocute\",\n",
    "     \"drown myself\",\"suffocate myself\", 'OD', \"overdose\", \"overdose on pills\", \"take pills\", \"swallow pills\", \"slit my wrists\", \"slash my arms\", \"cut myself\",\n",
    "     \"kill\", \"die\", \"hopeless\", \"useless\", \"pointless\", \"empty\", \"sad\", \"depressed\", \"cry\", \"cut\", \"hurt\", \"hate\", \"disappear\", \"vanish\", \"tired\", \"done\", \"end\", \"alone\",\n",
    "     \"broken\", \"slit\", \"drown\", \"strangle\", \"burn\", \"crash\", \"numb\", \"scared\", \"anxious\", \"panic\", \"afraid\", \"terrified\", \"angry\", \"furious\", \"rage\", \"mad\", \"can't go on\", \"no one cares\",\n",
    "     \"nobody cares\", \"why am i here\", \"not okay\", \"life sucks\",\n",
    "]\n",
    "\n",
    "found = [kw for kw in self_harm_keywords if kw in (transcript or \"\").lower()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚ö†Ô∏è Self-Harm Keyword Detection\")\n",
    "print(\"=\"*50)\n",
    "if found:\n",
    "    print(\"üö® Warning: Potential self-harm indicators found!\")\n",
    "    for kw in found:\n",
    "        print(f\" ‚Ä¢ Keyword Detected: \\\"{kw}\\\"\")\n",
    "    print(\"\\nüì¢ ACTION: Please escalate. Recommend contacting crisis support (e.g., Lifeline 13 11 14).\")\n",
    "else:\n",
    "    print(\" ‚Ä¢ No self-harm indicators detected.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
